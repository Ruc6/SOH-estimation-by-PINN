# -*- coding: utf-8 -*-
"""Week1 SOH

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zdMyQvXM7XhL_shmOThjGgHOHZ-qKHDX
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

def build_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(100, activation='tanh'),
        tf.keras.layers.Dense(100, activation='tanh'),
        tf.keras.layers.Dense(100, activation='tanh'),
        tf.keras.layers.Dense(100, activation='tanh'),
        tf.keras.layers.Dense(1)
    ])
    return model

def physics_loss(model, x):
    with tf.GradientTape() as tape:
        tape.watch(x)
        y_pred = model(x)

    dy_dx = tape.gradient(y_pred, x)

    residual = dy_dx + y_pred
    return tf.reduce_mean(tf.square(residual))

x_bc = tf.constant([[0.0]], dtype=tf.float32)
y_bc = tf.constant([[1.0]], dtype=tf.float32)

def boundary_loss(model):
    return tf.reduce_mean(tf.square(model(x_bc) - y_bc))

x_physics = tf.convert_to_tensor(np.linspace(0, 5, 100).reshape(-1, 1), dtype=tf.float32)

model = build_model()
optimizer = tf.keras.optimizers.Adam(0.001)
epochs = 2000
for epoch in range(epochs):
    with tf.GradientTape() as tape:
        loss_value = physics_loss(model, x_physics)
        loss_bc = boundary_loss(model)
        loss = loss_value + 10 * loss_bc
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))

    if epoch % 200 == 0:
        print(f"Epoch {epoch}, Physics Loss = {loss_value.numpy():.6f}")

x_test = np.linspace(0, 5, 200).reshape(-1, 1)
y_pred = model.predict(x_test)

plt.plot(x_test, np.exp(-x_test), label="True e^{-x}")
plt.plot(x_test, y_pred, label="PINN Pred")
plt.legend()
plt.show()

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

def build_model():
    return tf.keras.Sequential([
        tf.keras.layers.Dense(100, activation='tanh'),
        tf.keras.layers.Dense(100, activation='tanh'),
        tf.keras.layers.Dense(100, activation='tanh'),
        tf.keras.layers.Dense(1)
    ])

def physics_loss(model, x):
    with tf.GradientTape() as t2:
        t2.watch(x)
        with tf.GradientTape() as t1:
            t1.watch(x)
            y = model(x)
        dy = t1.gradient(y, x)
    d2y = t2.gradient(dy, x)

    omega = 10.0
    res = d2y + omega**2 * y
    return tf.reduce_mean(tf.square(res))

x_bc = tf.constant([[0.0],
                    [np.pi/20]], dtype=tf.float32)

y_bc = tf.constant([[0.0],
                    [1.0]], dtype=tf.float32)



def boundary_loss(model):
    return tf.reduce_mean(tf.square(model(x_bc) - y_bc))

model = build_model()
optimizer = tf.keras.optimizers.Adam(0.001)

x_train = tf.convert_to_tensor(
    np.linspace(0, 10, 200).reshape(-1, 1),
    dtype=tf.float32
)

epochs = 2000
for e in range(epochs):
    with tf.GradientTape() as tape:
        l_p = physics_loss(model, x_train)
        l_bc = boundary_loss(model)
        loss = l_p + 10 * l_bc

    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))

    if e % 200 == 0:
        print(f"Epoch {e}, Total Loss = {loss.numpy():.6f}, "
              f"Physics = {l_p.numpy():.6f}, BC = {l_bc.numpy():.6f}")

import numpy as np
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras import layers


# ================================
# 1. BUILD MODEL
# ================================
def build_model():
    model = keras.Sequential([
        layers.Dense(32, activation='tanh'),
        layers.Dense(32, activation='tanh'),
        layers.Dense(1)  # NO ACTIVATION
    ])
    return model

model = build_model()


# ================================
# 2. HARD ENFORCE BOUNDARY CONDITIONS
#    y(x) = 1 + x * N(x)
#    => automatically y(0)=1, y'(0)=0
# ================================
def pinn_output(model, x):
    # x: shape (batch,1)
    n = model(x)
    return 1.0 + x * n   # automatically meets BC


# ================================
# 3. PHYSICS LOSS
# ================================
def physics_loss(model, x):
    omega = 10.0

    with tf.GradientTape(persistent=True) as t2:
        t2.watch(x)
        with tf.GradientTape() as t1:
            t1.watch(x)
            y = pinn_output(model, x)
        dy = t1.gradient(y, x)
    d2y = t2.gradient(dy, x)
    del t2

    residual = d2y + omega**2 * y
    return tf.reduce_mean(tf.square(residual))


# ================================
# 4. TRAINING SETUP
# ================================
optimizer = tf.keras.optimizers.Adam(1e-3)

# Physics points
x_train = tf.convert_to_tensor(
    np.linspace(0, 10, 200).reshape(-1, 1),
    dtype=tf.float32
)


# ================================
# 5. TRAIN LOOP
# ================================
epochs = 3000

for e in range(epochs):
    with tf.GradientTape() as tape:
        lp = physics_loss(model, x_train)
        loss = lp  # no BC loss needed (hard-enforced)

    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))

    if e % 200 == 0:
        print(f"Epoch {e}, Physics Loss = {lp.numpy():.6f}")

x_test = np.linspace(0, 10, 400).reshape(-1, 1)
y_pred = model.predict(x_test)

omega = 10
y_true = np.sin(omega * x_test)

plt.plot(x_test, y_true, label="True sin(10x)")
plt.plot(x_test, y_pred, label="PINN Pred ")
plt.legend()
plt.show()

import tensorflow as tf
import numpy as np

t_train = np.linspace(0, 4*np.pi, 100)[:, None].astype(np.float32)

y_train = np.sin(t_train)

t_bc = np.array([[0.0], [np.pi/2], [np.pi]], dtype=np.float32)
y_bc = np.array([[0.0], [1.0], [0.0]], dtype=np.float32)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers

class SHMPINN(tf.keras.Model):
    def __init__(self):
        super(SHMPINN, self).__init__()

        self.net = Sequential([
            layers.Input(shape=(1,)),
            layers.Dense(20, activation='tanh'),
            layers.Dense(20, activation='tanh'),
            layers.Dense(1, activation=None)
        ])

    def call(self, t):
        return self.net(t)

def physics_residual(model, t):
    with tf.GradientTape(persistent=True) as tape2:
        tape2.watch(t)
        with tf.GradientTape() as tape1:
            tape1.watch(t)
            y = model(t)
        y_t = tape1.gradient(y, t)
    y_tt = tape2.gradient(y_t, t)
    del tape2
    return y_tt + y

def pinn_loss(model, t_train, t_bc, y_bc):
    t_train_tf = tf.convert_to_tensor(t_train)
    f = physics_residual(model, t_train_tf)
    physics_loss = tf.reduce_mean(tf.square(f))

    t_bc_tf = tf.convert_to_tensor(t_bc)
    y_bc_tf = tf.convert_to_tensor(y_bc)
    y_pred_bc = model(t_bc_tf)
    bc_loss = tf.reduce_mean(tf.square(y_pred_bc - y_bc_tf))

    return physics_loss + bc_loss

model = SHMPINN()
optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
epochs = 3000

for epoch in range(epochs):
    with tf.GradientTape() as tape:
        loss = pinn_loss(model, t_train, t_bc, y_bc)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))

    if epoch % 300 == 0:
        print(f"Epoch {epoch}, Loss: {loss.numpy():.6f}")

import matplotlib.pyplot as plt

t_test = np.linspace(0, 4*np.pi, 200)[:, None].astype(np.float32)
y_pred = model(t_test).numpy()

plt.plot(t_test, np.sin(t_test), label='Analytical')
plt.plot(t_test, y_pred, '--', label='PINN Prediction')
plt.scatter(t_bc, y_bc, color='red', label='BC Points')
plt.legend()
plt.show()

